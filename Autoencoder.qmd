---
title: SVD
format:
  html:
    code-fold: true
    page-layout: full
---


# Import libraries
Start by importing pytorch & matplotlib. I could not make the latex interpreter work in binder, so the corresponding lines are commented out. 

```{python}
import torch 
import torch.nn as nn
import matplotlib.pyplot as plt
plt.rcParams['svg.fonttype'] = 'none'
from IPython.display import set_matplotlib_formats
import matplotlib
import numpy as np
#%% matplotlib setup
matplotlib.rcParams["text.usetex"] = True
matplotlib.rcParams["font.family"] = "serif"
matplotlib.rcParams["font.size"] = "14"
mps_device = torch.device("mps")
```

# Create the functions

The separability proprieties of several functions are investigated. This code first shows the separability proprieties of different ways of clustering the 1D space into two regions. 
Two types of functions are used to do so:
* A sharp step function (Heaviside),
* A smooth one (Tanh).

In both cases the position of the jump is parametrised by a scalar parameter $\alpha$. 

Furhter investigations are conducted on moving front(s) with a 
* A gaussian function that is moving 
* Two gaussian functions moving at different rates

```{python}
L = 10                                      # Space domain
Alpha_vect = torch.linspace(0,1,100)        # vector of alphas
x_vect = torch.linspace(0,L,2000)           # vector of x


Function = 'Heaviside'                      # Alpha-parameterised step function
Function = 'Tanh'                           # smooth alpha-parameterised step function
# Function = 'Gauss'                        # Alpha-parameterised front function
# Function = 'Gauss_sum'                      # Double alpha-parameterised front functions




if Function == 'Heaviside':
    F = torch.heaviside((x_vect[:,None] - (1-Alpha_vect[None,:])*L), x_vect[-1]/x_vect[-1])
elif Function == 'Tanh':
    F = torch.tanh((x_vect[:,None] - (1-Alpha_vect[None,:])*L))
elif Function == 'Gauss':
    F = torch.exp(-(x_vect[:,None] - (1-Alpha_vect[None,:])*L)**2)

elif Function == 'Gauss_sum':
    F = torch.exp(-(x_vect[:,None] - (1-Alpha_vect[None,:])*L)**2) + torch.exp(-(x_vect[:,None] - (1-2*Alpha_vect[None,:])*L)**2) 



```

# Define the autoencoder

```{python}


class AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.N_0 = 2000
        self.N_1 = 1000
        self.N_2 = 200
        self.N_3 = 1
        self.encoder = nn.Sequential(
        nn.Linear(self.N_0,self.N_1),
        nn.Tanh(),
        nn.Linear(self.N_1, self.N_2),
        nn.Tanh(),
        nn.Linear(self.N_2, self.N_3)
        )

        self.decoder = nn.Sequential(
        nn.Linear(self.N_3,self.N_2),
        nn.ReLU(),
        nn.Linear(self.N_2, self.N_1),
        nn.ReLU(),
        nn.Linear(self.N_1, self.N_0)
        )
    
    def forward(self, x, role = "decode"):
        if self.training:
            e = self.encoder(x)
            d = self.decoder(e)
            return d
        else:
            match role:
                case "encode":
                    e = self.encoder(x)
                    return e
                case "decode":
                    d = self.decoder(x)
                    return d

ROM = AutoEncoder()
MSE = nn.MSELoss()

optimizer = torch.optim.Adam(ROM.parameters(),
                             lr = 1e-3)

```


# Compute its SVD

```{python}
U, S, V = torch.svd(F)

```

# Create validation and training sets

```{python}


n_epochs = 400
import random
val = int(np.floor(0.2*F.shape[1]))
F_train = F
for n in range(val):
    r = random.randint(1, F_train.shape[1]-1)
    F_val = F_train[:,r]
    F_train = torch.cat([F_train[:, :r], F_train[:, r+1:]], dim=1)

F_train = F_train.T
F_val = F_val.T

loss_t_vect = []
loss_v_vect = []

```

# Train the model

```{python}
#| echo: true

ROM.train()
F_train = F_train.to(mps_device)
F_val = F_val.to(mps_device)
ROM.to(mps_device)
for epochs in range(n_epochs):
    loss = MSE(ROM(F_train),F_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_t_vect.append(loss.data)
    loss_v_vect.append(MSE(ROM(F_val),F_val).data)
    print(f'epoch {epochs+1} loss = {np.format_float_scientific(loss.item(), precision=4)}')


```

# Evaluate the model

```{python}


ROM.eval()

F_train = F_train.cpu()
F_val = F_val.cpu()
ROM.cpu()
torch.save(ROM, 'FullModel.pt') # to save a full coarse model

```

# Plots


```{python}


loss_t_vect = [loss_t.cpu() for loss_t in loss_t_vect]
loss_v_vect = [loss_v.cpu() for loss_v in loss_v_vect]


plt.plot(loss_t_vect,label = 'training set')
plt.plot(loss_v_vect,label = 'validation set')
plt.legend(loc="upper right")
plt.show()
plt.semilogy(loss_t_vect,label = 'training set')
plt.semilogy(loss_v_vect,label = 'validation set')
plt.legend(loc="upper right")
plt.xlabel('Epochs')
plt.xlabel('Loss')
# plt.savefig(f'Results/loss_training_'+Function+'.pdf', transparent=True)  
plt.show()


```

# Plot the comparison of the latent space and the natural parameter used to generate the data

```{python}


Alpha_latent = ROM(F.t(),"encode")
plt.plot(Alpha_vect.view(-1,1).cpu().data,Alpha_latent.view(-1,1).cpu().data)
plt.xlabel(r'$\alpha$')
plt.ylabel(r'$\hat{\alpha}$')
plt.show()


```


# Plot reconstructed image

```{python}


F_reconstructed = ROM(Alpha_latent.view(-1,1))
# F_reconstructed = ROM(Alpha_vect.view(-1,1))

plt.imshow(F_reconstructed.cpu().data,cmap='gray')
plt.show()


```

# Plot errors

## Error map

```{python}

plt.imshow(F_reconstructed.cpu().data-F.t(),cmap='gray')
plt.show()



```


## Errors on sliced fields

The trained model and the reference are compared for two values
* $\alpha = 1/3$ &
* $\alpha = 2/3$.

```{python}

idx1 = int(np.floor(0.66*F.shape[1]))
idx2 = int(np.floor(0.33*F.shape[1]))

plt.plot(x_vect,F[:,idx1],'k',label='Full, alpha = 2/3')
plt.plot(x_vect,F_reconstructed.t()[:,idx1].cpu().data,'--',label='Truncated, alpha = 2/3')
plt.plot(x_vect,F[:,idx2],'k',label='Full, alpha = 1/3')
plt.plot(x_vect,F_reconstructed.t()[:,idx2].cpu().data,'--',label='Truncated, alpha = 1/3')
plt.legend(loc="upper left")
plt.title(f'2 slices of the field')
plt.xlabel(r'$x$')
plt.ylabel(r'$f(x,\alpha)$')
# plt.savefig(f'Results/Sliced_TruncatedField_{N}_'+Function+'.pdf', transparent=True)  
plt.show()
```