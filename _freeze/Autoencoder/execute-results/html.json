{
  "hash": "dc07d727ced94fe113a2a527f2b9c0e7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Autoencoder\nformat:\n  html:\n    code-fold: true\n    page-layout: full\n---\n\n\n\n\n\n# Import libraries\nStart by importing pytorch & matplotlib. I could not make the latex interpreter work in binder, so the corresponding lines are commented out. \n\n::: {#cf8b4ae2 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nplt.rcParams['svg.fonttype'] = 'none'\nfrom IPython.display import set_matplotlib_formats\nimport matplotlib\nimport numpy as np\n#%% matplotlib setup\nmatplotlib.rcParams[\"text.usetex\"] = True\nmatplotlib.rcParams[\"font.family\"] = \"serif\"\nmatplotlib.rcParams[\"font.size\"] = \"14\"\nmps_device = torch.device(\"mps\")\n```\n:::\n\n\n# Create the functions\n\nThe separability proprieties of several functions are investigated. This code first shows the separability proprieties of different ways of clustering the 1D space into two regions. \nTwo types of functions are used to do so:\n\n* A sharp step function (Heaviside),\n* A smooth one (Tanh).\n\nIn both cases the position of the jump is parametrised by a scalar parameter $\\alpha$. \n\nFurhter investigations are conducted on moving front(s) with a \n\n* A gaussian function that is moving \n* Two gaussian functions moving at different rates\n\n::: {#793d7b0a .cell execution_count=2}\n``` {.python .cell-code}\nL = 10                                      # Space domain\nAlpha_vect = torch.linspace(0,1,100)        # vector of alphas\nx_vect = torch.linspace(0,L,2000)           # vector of x\n\n\nFunction = 'Heaviside'                      # Alpha-parameterised step function\nFunction = 'Tanh'                           # smooth alpha-parameterised step function\n# Function = 'Gauss'                        # Alpha-parameterised front function\n# Function = 'Gauss_sum'                      # Double alpha-parameterised front functions\n\n\n\n\nif Function == 'Heaviside':\n    F = torch.heaviside((x_vect[:,None] - (1-Alpha_vect[None,:])*L), x_vect[-1]/x_vect[-1])\nelif Function == 'Tanh':\n    F = torch.tanh((x_vect[:,None] - (1-Alpha_vect[None,:])*L))\nelif Function == 'Gauss':\n    F = torch.exp(-(x_vect[:,None] - (1-Alpha_vect[None,:])*L)**2)\n\nelif Function == 'Gauss_sum':\n    F = torch.exp(-(x_vect[:,None] - (1-Alpha_vect[None,:])*L)**2) + torch.exp(-(x_vect[:,None] - (1-2*Alpha_vect[None,:])*L)**2) \n\n```\n:::\n\n\n# Define the autoencoder\n\nWe know that the parametrised field is (non-linearly) parametrised with a single parameter. The SVD required approximatively $15$ modes to reprensent the field. We propose to create an autoencoder with a single parameter in the latent space. *i.e.* $N_3=1$, to see how the non-linear interpolation can retrive the single parameter dependency of the function.\n\n::: {#39f301a5 .cell execution_count=3}\n``` {.python .cell-code}\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.N_0 = 2000\n        self.N_1 = 1000\n        self.N_2 = 200\n        self.N_3 = 1\n        self.encoder = nn.Sequential(\n        nn.Linear(self.N_0,self.N_1),\n        nn.Tanh(),\n        nn.Linear(self.N_1, self.N_2),\n        nn.Tanh(),\n        nn.Linear(self.N_2, self.N_3)\n        )\n\n        self.decoder = nn.Sequential(\n        nn.Linear(self.N_3,self.N_2),\n        nn.ReLU(),\n        nn.Linear(self.N_2, self.N_1),\n        nn.ReLU(),\n        nn.Linear(self.N_1, self.N_0)\n        )\n    \n    def forward(self, x, role = \"decode\"):\n        if self.training:\n            e = self.encoder(x)\n            d = self.decoder(e)\n            return d\n        else:\n            match role:\n                case \"encode\":\n                    e = self.encoder(x)\n                    return e\n                case \"decode\":\n                    d = self.decoder(x)\n                    return d\n\nROM = AutoEncoder()\nMSE = nn.MSELoss()\n\noptimizer = torch.optim.Adam(ROM.parameters(),\n                             lr = 1e-3)\n```\n:::\n\n\n# Create validation and training sets\n\n::: {#74c30f7a .cell execution_count=4}\n``` {.python .cell-code}\nn_epochs = 400\nimport random\nval = int(np.floor(0.2*F.shape[1]))\nF_train = F\nfor n in range(val):\n    r = random.randint(1, F_train.shape[1]-1)\n    F_val = F_train[:,r]\n    F_train = torch.cat([F_train[:, :r], F_train[:, r+1:]], dim=1)\n\nF_train = F_train.T\nF_val = F_val.T\n\nloss_t_vect = []\nloss_v_vect = []\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/vt/tk0pvn6x7dj443pvlvkxzcj00000gn/T/ipykernel_95795/3940643703.py:11: UserWarning:\n\nThe use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/TensorShape.cpp:3618.)\n\n```\n:::\n:::\n\n\n# Train the model\n\n::: {#6b3dffb6 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch 1 loss = 8.2155e-01\nepoch 2 loss = 7.4085e-01\nepoch 3 loss = 6.4534e-01\nepoch 4 loss = 4.8344e-01\nepoch 5 loss = 4.8184e-01\nepoch 6 loss = 4.5387e-01\nepoch 7 loss = 3.9579e-01\nepoch 8 loss = 3.7095e-01\nepoch 9 loss = 3.3925e-01\nepoch 10 loss = 3.0197e-01\nepoch 11 loss = 2.8023e-01\nepoch 12 loss = 2.5817e-01\nepoch 13 loss = 2.4296e-01\nepoch 14 loss = 2.3631e-01\nepoch 15 loss = 2.271e-01\nepoch 16 loss = 2.1174e-01\nepoch 17 loss = 2.0173e-01\nepoch 18 loss = 1.9554e-01\nepoch 19 loss = 1.8771e-01\nepoch 20 loss = 1.7975e-01\nepoch 21 loss = 1.6976e-01\nepoch 22 loss = 1.6103e-01\nepoch 23 loss = 1.5026e-01\nepoch 24 loss = 1.3971e-01\nepoch 25 loss = 1.2882e-01\nepoch 26 loss = 1.1729e-01\nepoch 27 loss = 1.065e-01\nepoch 28 loss = 9.6526e-02\nepoch 29 loss = 8.6951e-02\nepoch 30 loss = 7.8627e-02\nepoch 31 loss = 7.0401e-02\nepoch 32 loss = 6.3924e-02\nepoch 33 loss = 5.8911e-02\nepoch 34 loss = 5.5006e-02\nepoch 35 loss = 5.1995e-02\nepoch 36 loss = 5.0566e-02\nepoch 37 loss = 4.8634e-02\nepoch 38 loss = 4.7474e-02\nepoch 39 loss = 4.7193e-02\nepoch 40 loss = 4.7940e-02\nepoch 41 loss = 5.2896e-02\nepoch 42 loss = 5.9132e-02\nepoch 43 loss = 4.8067e-02\nepoch 44 loss = 4.2177e-02\nepoch 45 loss = 4.8879e-02\nepoch 46 loss = 3.9875e-02\nepoch 47 loss = 4.2604e-02\nepoch 48 loss = 4.1113e-02\nepoch 49 loss = 3.8979e-02\nepoch 50 loss = 3.9686e-02\nepoch 51 loss = 3.7978e-02\nepoch 52 loss = 3.7426e-02\nepoch 53 loss = 3.6782e-02\nepoch 54 loss = 3.5568e-02\nepoch 55 loss = 3.4666e-02\nepoch 56 loss = 3.3975e-02\nepoch 57 loss = 3.2523e-02\nepoch 58 loss = 3.2201e-02\nepoch 59 loss = 3.0572e-02\nepoch 60 loss = 3.0382e-02\nepoch 61 loss = 2.8892e-02\nepoch 62 loss = 2.8115e-02\nepoch 63 loss = 2.7071e-02\nepoch 64 loss = 2.5838e-02\nepoch 65 loss = 2.4633e-02\nepoch 66 loss = 2.369e-02\nepoch 67 loss = 2.225e-02\nepoch 68 loss = 2.1503e-02\nepoch 69 loss = 2.0067e-02\nepoch 70 loss = 1.9367e-02\nepoch 71 loss = 1.8188e-02\nepoch 72 loss = 1.7298e-02\nepoch 73 loss = 1.6423e-02\nepoch 74 loss = 1.5630e-02\nepoch 75 loss = 1.4770e-02\nepoch 76 loss = 1.4277e-02\nepoch 77 loss = 1.3544e-02\nepoch 78 loss = 1.3142e-02\nepoch 79 loss = 1.266e-02\nepoch 80 loss = 1.2276e-02\nepoch 81 loss = 1.1928e-02\nepoch 82 loss = 1.1651e-02\nepoch 83 loss = 1.132e-02\nepoch 84 loss = 1.1146e-02\nepoch 85 loss = 1.0812e-02\nepoch 86 loss = 1.0640e-02\nepoch 87 loss = 1.0346e-02\nepoch 88 loss = 1.0115e-02\nepoch 89 loss = 9.8678e-03\nepoch 90 loss = 9.5919e-03\nepoch 91 loss = 9.3518e-03\nepoch 92 loss = 9.0759e-03\nepoch 93 loss = 8.8265e-03\nepoch 94 loss = 8.5736e-03\nepoch 95 loss = 8.3326e-03\nepoch 96 loss = 8.0972e-03\nepoch 97 loss = 7.8782e-03\nepoch 98 loss = 7.6577e-03\nepoch 99 loss = 7.4597e-03\nepoch 100 loss = 7.2663e-03\nepoch 101 loss = 7.0822e-03\nepoch 102 loss = 6.9131e-03\nepoch 103 loss = 6.7434e-03\nepoch 104 loss = 6.5875e-03\nepoch 105 loss = 6.4347e-03\nepoch 106 loss = 6.2886e-03\nepoch 107 loss = 6.1507e-03\nepoch 108 loss = 6.0154e-03\nepoch 109 loss = 5.8867e-03\nepoch 110 loss = 5.7649e-03\nepoch 111 loss = 5.6437e-03\nepoch 112 loss = 5.5332e-03\nepoch 113 loss = 5.4245e-03\nepoch 114 loss = 5.3208e-03\nepoch 115 loss = 5.2267e-03\nepoch 116 loss = 5.1309e-03\nepoch 117 loss = 5.0453e-03\nepoch 118 loss = 4.9602e-03\nepoch 119 loss = 4.8775e-03\nepoch 120 loss = 4.8005e-03\nepoch 121 loss = 4.7221e-03\nepoch 122 loss = 4.6473e-03\nepoch 123 loss = 4.5739e-03\nepoch 124 loss = 4.5008e-03\nepoch 125 loss = 4.4290e-03\nepoch 126 loss = 4.3590e-03\nepoch 127 loss = 4.2876e-03\nepoch 128 loss = 4.2188e-03\nepoch 129 loss = 4.1503e-03\nepoch 130 loss = 4.0826e-03\nepoch 131 loss = 4.0159e-03\nepoch 132 loss = 3.9502e-03\nepoch 133 loss = 3.8851e-03\nepoch 134 loss = 3.8207e-03\nepoch 135 loss = 3.7579e-03\nepoch 136 loss = 3.6948e-03\nepoch 137 loss = 3.6329e-03\nepoch 138 loss = 3.5718e-03\nepoch 139 loss = 3.5115e-03\nepoch 140 loss = 3.4516e-03\nepoch 141 loss = 3.3928e-03\nepoch 142 loss = 3.3345e-03\nepoch 143 loss = 3.2768e-03\nepoch 144 loss = 3.2201e-03\nepoch 145 loss = 3.1643e-03\nepoch 146 loss = 3.1091e-03\nepoch 147 loss = 3.0549e-03\nepoch 148 loss = 3.0017e-03\nepoch 149 loss = 2.9495e-03\nepoch 150 loss = 2.8982e-03\nepoch 151 loss = 2.848e-03\nepoch 152 loss = 2.7988e-03\nepoch 153 loss = 2.7507e-03\nepoch 154 loss = 2.7036e-03\nepoch 155 loss = 2.6574e-03\nepoch 156 loss = 2.6121e-03\nepoch 157 loss = 2.5678e-03\nepoch 158 loss = 2.5244e-03\nepoch 159 loss = 2.4818e-03\nepoch 160 loss = 2.4402e-03\nepoch 161 loss = 2.3995e-03\nepoch 162 loss = 2.3597e-03\nepoch 163 loss = 2.321e-03\nepoch 164 loss = 2.2831e-03\nepoch 165 loss = 2.2461e-03\nepoch 166 loss = 2.2100e-03\nepoch 167 loss = 2.1748e-03\nepoch 168 loss = 2.1405e-03\nepoch 169 loss = 2.1068e-03\nepoch 170 loss = 2.074e-03\nepoch 171 loss = 2.042e-03\nepoch 172 loss = 2.0107e-03\nepoch 173 loss = 1.9802e-03\nepoch 174 loss = 1.9505e-03\nepoch 175 loss = 1.9215e-03\nepoch 176 loss = 1.8931e-03\nepoch 177 loss = 1.8654e-03\nepoch 178 loss = 1.8384e-03\nepoch 179 loss = 1.8120e-03\nepoch 180 loss = 1.7862e-03\nepoch 181 loss = 1.7610e-03\nepoch 182 loss = 1.7363e-03\nepoch 183 loss = 1.7122e-03\nepoch 184 loss = 1.6885e-03\nepoch 185 loss = 1.6653e-03\nepoch 186 loss = 1.6427e-03\nepoch 187 loss = 1.6205e-03\nepoch 188 loss = 1.5989e-03\nepoch 189 loss = 1.5778e-03\nepoch 190 loss = 1.5573e-03\nepoch 191 loss = 1.5379e-03\nepoch 192 loss = 1.5206e-03\nepoch 193 loss = 1.5071e-03\nepoch 194 loss = 1.5021e-03\nepoch 195 loss = 1.5185e-03\nepoch 196 loss = 1.5856e-03\nepoch 197 loss = 1.7829e-03\nepoch 198 loss = 2.2682e-03\nepoch 199 loss = 3.3796e-03\nepoch 200 loss = 5.2142e-03\nepoch 201 loss = 6.7241e-03\nepoch 202 loss = 4.8273e-03\nepoch 203 loss = 1.7260e-03\nepoch 204 loss = 2.0891e-03\nepoch 205 loss = 3.6923e-03\nepoch 206 loss = 2.2138e-03\nepoch 207 loss = 1.4608e-03\nepoch 208 loss = 2.8068e-03\nepoch 209 loss = 1.9051e-03\nepoch 210 loss = 1.4191e-03\nepoch 211 loss = 2.3544e-03\nepoch 212 loss = 1.4942e-03\nepoch 213 loss = 1.4848e-03\nepoch 214 loss = 1.9594e-03\nepoch 215 loss = 1.2159e-03\nepoch 216 loss = 1.5578e-03\nepoch 217 loss = 1.5672e-03\nepoch 218 loss = 1.137e-03\nepoch 219 loss = 1.5317e-03\nepoch 220 loss = 1.2545e-03\nepoch 221 loss = 1.1778e-03\nepoch 222 loss = 1.3977e-03\nepoch 223 loss = 1.0851e-03\nepoch 224 loss = 1.2199e-03\nepoch 225 loss = 1.2228e-03\nepoch 226 loss = 1.0328e-03\nepoch 227 loss = 1.2000e-03\nepoch 228 loss = 1.0747e-03\nepoch 229 loss = 1.0323e-03\nepoch 230 loss = 1.1263e-03\nepoch 231 loss = 9.8404e-04\nepoch 232 loss = 1.0296e-03\nepoch 233 loss = 1.0345e-03\nepoch 234 loss = 9.4081e-04\nepoch 235 loss = 1.0055e-03\nepoch 236 loss = 9.5542e-04\nepoch 237 loss = 9.2134e-04\nepoch 238 loss = 9.6229e-04\nepoch 239 loss = 8.9963e-04\nepoch 240 loss = 9.0467e-04\nepoch 241 loss = 9.1333e-04\nepoch 242 loss = 8.6303e-04\nepoch 243 loss = 8.8175e-04\nepoch 244 loss = 8.6798e-04\nepoch 245 loss = 8.3659e-04\nepoch 246 loss = 8.5344e-04\nepoch 247 loss = 8.2997e-04\nepoch 248 loss = 8.1350e-04\nepoch 249 loss = 8.2295e-04\nepoch 250 loss = 7.9848e-04\nepoch 251 loss = 7.9071e-04\nepoch 252 loss = 7.9310e-04\nepoch 253 loss = 7.7155e-04\nepoch 254 loss = 7.6778e-04\nepoch 255 loss = 7.6541e-04\nepoch 256 loss = 7.4756e-04\nepoch 257 loss = 7.4496e-04\nepoch 258 loss = 7.3986e-04\nepoch 259 loss = 7.2529e-04\nepoch 260 loss = 7.2263e-04\nepoch 261 loss = 7.1634e-04\nepoch 262 loss = 7.0429e-04\nepoch 263 loss = 7.0100e-04\nepoch 264 loss = 6.9452e-04\nepoch 265 loss = 6.8423e-04\nepoch 266 loss = 6.8027e-04\nepoch 267 loss = 6.7406e-04\nepoch 268 loss = 6.6498e-04\nepoch 269 loss = 6.605e-04\nepoch 270 loss = 6.5475e-04\nepoch 271 loss = 6.4654e-04\nepoch 272 loss = 6.4166e-04\nepoch 273 loss = 6.3633e-04\nepoch 274 loss = 6.2891e-04\nepoch 275 loss = 6.2364e-04\nepoch 276 loss = 6.1873e-04\nepoch 277 loss = 6.1203e-04\nepoch 278 loss = 6.0647e-04\nepoch 279 loss = 6.0177e-04\nepoch 280 loss = 5.9577e-04\nepoch 281 loss = 5.9010e-04\nepoch 282 loss = 5.8548e-04\nepoch 283 loss = 5.8009e-04\nepoch 284 loss = 5.7451e-04\nepoch 285 loss = 5.6982e-04\nepoch 286 loss = 5.6495e-04\nepoch 287 loss = 5.596e-04\nepoch 288 loss = 5.5484e-04\nepoch 289 loss = 5.503e-04\nepoch 290 loss = 5.4535e-04\nepoch 291 loss = 5.4053e-04\nepoch 292 loss = 5.3614e-04\nepoch 293 loss = 5.3158e-04\nepoch 294 loss = 5.2687e-04\nepoch 295 loss = 5.2250e-04\nepoch 296 loss = 5.1821e-04\nepoch 297 loss = 5.1374e-04\nepoch 298 loss = 5.0938e-04\nepoch 299 loss = 5.0527e-04\nepoch 300 loss = 5.0108e-04\nepoch 301 loss = 4.9686e-04\nepoch 302 loss = 4.9278e-04\nepoch 303 loss = 4.8881e-04\nepoch 304 loss = 4.8479e-04\nepoch 305 loss = 4.8081e-04\nepoch 306 loss = 4.7694e-04\nepoch 307 loss = 4.7312e-04\nepoch 308 loss = 4.6928e-04\nepoch 309 loss = 4.6549e-04\nepoch 310 loss = 4.6178e-04\nepoch 311 loss = 4.5811e-04\nepoch 312 loss = 4.5443e-04\nepoch 313 loss = 4.5081e-04\nepoch 314 loss = 4.4727e-04\nepoch 315 loss = 4.4373e-04\nepoch 316 loss = 4.4021e-04\nepoch 317 loss = 4.3674e-04\nepoch 318 loss = 4.3334e-04\nepoch 319 loss = 4.2996e-04\nepoch 320 loss = 4.2660e-04\nepoch 321 loss = 4.2328e-04\nepoch 322 loss = 4.2001e-04\nepoch 323 loss = 4.1677e-04\nepoch 324 loss = 4.1357e-04\nepoch 325 loss = 4.1038e-04\nepoch 326 loss = 4.0725e-04\nepoch 327 loss = 4.0414e-04\nepoch 328 loss = 4.0106e-04\nepoch 329 loss = 3.9801e-04\nepoch 330 loss = 3.9499e-04\nepoch 331 loss = 3.9202e-04\nepoch 332 loss = 3.8906e-04\nepoch 333 loss = 3.8614e-04\nepoch 334 loss = 3.8324e-04\nepoch 335 loss = 3.8037e-04\nepoch 336 loss = 3.7753e-04\nepoch 337 loss = 3.7472e-04\nepoch 338 loss = 3.7196e-04\nepoch 339 loss = 3.6919e-04\nepoch 340 loss = 3.6647e-04\nepoch 341 loss = 3.6377e-04\nepoch 342 loss = 3.611e-04\nepoch 343 loss = 3.5846e-04\nepoch 344 loss = 3.5584e-04\nepoch 345 loss = 3.5326e-04\nepoch 346 loss = 3.5068e-04\nepoch 347 loss = 3.4814e-04\nepoch 348 loss = 3.4562e-04\nepoch 349 loss = 3.4313e-04\nepoch 350 loss = 3.4066e-04\nepoch 351 loss = 3.3822e-04\nepoch 352 loss = 3.3582e-04\nepoch 353 loss = 3.3341e-04\nepoch 354 loss = 3.3104e-04\nepoch 355 loss = 3.2868e-04\nepoch 356 loss = 3.2635e-04\nepoch 357 loss = 3.2404e-04\nepoch 358 loss = 3.2174e-04\nepoch 359 loss = 3.1948e-04\nepoch 360 loss = 3.1722e-04\nepoch 361 loss = 3.1499e-04\nepoch 362 loss = 3.1276e-04\nepoch 363 loss = 3.1055e-04\nepoch 364 loss = 3.0836e-04\nepoch 365 loss = 3.0618e-04\nepoch 366 loss = 3.0402e-04\nepoch 367 loss = 3.0186e-04\nepoch 368 loss = 2.9972e-04\nepoch 369 loss = 2.9759e-04\nepoch 370 loss = 2.9548e-04\nepoch 371 loss = 2.9336e-04\nepoch 372 loss = 2.9124e-04\nepoch 373 loss = 2.8913e-04\nepoch 374 loss = 2.8703e-04\nepoch 375 loss = 2.8494e-04\nepoch 376 loss = 2.8285e-04\nepoch 377 loss = 2.8079e-04\nepoch 378 loss = 2.7875e-04\nepoch 379 loss = 2.7671e-04\nepoch 380 loss = 2.7469e-04\nepoch 381 loss = 2.7273e-04\nepoch 382 loss = 2.7078e-04\nepoch 383 loss = 2.6885e-04\nepoch 384 loss = 2.6693e-04\nepoch 385 loss = 2.6504e-04\nepoch 386 loss = 2.6316e-04\nepoch 387 loss = 2.6129e-04\nepoch 388 loss = 2.5944e-04\nepoch 389 loss = 2.576e-04\nepoch 390 loss = 2.5578e-04\nepoch 391 loss = 2.5398e-04\nepoch 392 loss = 2.522e-04\nepoch 393 loss = 2.5043e-04\nepoch 394 loss = 2.4868e-04\nepoch 395 loss = 2.4696e-04\nepoch 396 loss = 2.4524e-04\nepoch 397 loss = 2.4356e-04\nepoch 398 loss = 2.419e-04\nepoch 399 loss = 2.4025e-04\nepoch 400 loss = 2.3863e-04\n```\n:::\n:::\n\n\n# Evaluate the model\n\n::: {#0648bdc7 .cell execution_count=6}\n``` {.python .cell-code}\nROM.eval()\n\nF_train = F_train.cpu()\nF_val = F_val.cpu()\nROM.cpu()\ntorch.save(ROM, 'FullModel.pt') # to save a full coarse model\n```\n:::\n\n\n# Plots\n\n::: {#27fcf7bb .cell execution_count=7}\n``` {.python .cell-code}\nloss_t_vect = [loss_t.cpu() for loss_t in loss_t_vect]\nloss_v_vect = [loss_v.cpu() for loss_v in loss_v_vect]\n\n\nplt.plot(loss_t_vect,label = 'training set')\nplt.plot(loss_v_vect,label = 'validation set')\nplt.legend(loc=\"upper right\")\nplt.show()\nplt.semilogy(loss_t_vect,label = 'training set')\nplt.semilogy(loss_v_vect,label = 'validation set')\nplt.legend(loc=\"upper right\")\nplt.xlabel('Epochs')\nplt.xlabel('Loss')\n# plt.savefig(f'Results/loss_training_'+Function+'.pdf', transparent=True)  \nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-8-output-1.png){width=573 height=415}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-8-output-2.png){width=585 height=437}\n:::\n:::\n\n\n# Plot the comparison of the latent space and the natural parameter used to generate the data\n\n::: {#8d51e632 .cell execution_count=8}\n``` {.python .cell-code}\nAlpha_latent = ROM(F.t(),\"encode\")\nplt.plot(Alpha_vect.view(-1,1).cpu().data,Alpha_latent.view(-1,1).cpu().data)\nplt.xlabel(r'$\\alpha$')\nplt.ylabel(r'$\\hat{\\alpha}$')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-9-output-1.png){width=595 height=437}\n:::\n:::\n\n\n# Plot reconstructed image\n\n::: {#902b8334 .cell execution_count=9}\n``` {.python .cell-code}\nF_reconstructed = ROM(Alpha_latent.view(-1,1))\n# F_reconstructed = ROM(Alpha_vect.view(-1,1))\n\nplt.imshow(F_reconstructed.cpu().data,cmap='gray')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-10-output-1.png){width=568 height=77}\n:::\n:::\n\n\n# Plot errors\n\n## Error map\n\n::: {#067b6820 .cell execution_count=10}\n``` {.python .cell-code}\nplt.imshow(F_reconstructed.cpu().data-F.t(),cmap='gray')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-11-output-1.png){width=568 height=77}\n:::\n:::\n\n\n## Errors on sliced fields\n\nThe trained model and the reference are compared for two values\n\n* $\\alpha = 1/3$ &\n* $\\alpha = 2/3$.\n\n::: {#e2982ac3 .cell execution_count=11}\n``` {.python .cell-code}\nidx1 = int(np.floor(0.66*F.shape[1]))\nidx2 = int(np.floor(0.33*F.shape[1]))\n\nplt.plot(x_vect,F[:,idx1],'k',label='Full, alpha = 2/3')\nplt.plot(x_vect,F_reconstructed.t()[:,idx1].cpu().data,'--',label='Truncated, alpha = 2/3')\nplt.plot(x_vect,F[:,idx2],'k',label='Full, alpha = 1/3')\nplt.plot(x_vect,F_reconstructed.t()[:,idx2].cpu().data,'--',label='Truncated, alpha = 1/3')\nplt.legend(loc=\"upper left\")\nplt.title(f'2 slices of the field')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$f(x,\\alpha)$')\n# plt.savefig(f'Results/Sliced_TruncatedField_{N}_'+Function+'.pdf', transparent=True)  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Autoencoder_files/figure-html/cell-12-output-1.png){width=620 height=460}\n:::\n:::\n\n\nWith only one latent space-parameter the error are reasonably low. Using non-linear interpolation is more appropriate when the separability is low unsing linear subspaces. \n\n",
    "supporting": [
      "Autoencoder_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}